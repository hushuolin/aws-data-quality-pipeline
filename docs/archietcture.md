# Project Overview for Data Quality Assurance Pipeline

## Project Background
This project aims to implement a **Data Quality Assurance Pipeline** for a fintech company that processes large volumes of transaction data. The pipeline ensures the quality of data ingested from various sources into the data platform by detecting schema changes, validating data, and notifying relevant stakeholders of any inconsistencies. Given the complexity of financial data, maintaining data accuracy and consistency is critical for ensuring the integrity of business processes and compliance with industry regulations.

## Objectives
The primary objectives of the data quality assurance pipeline include:
- **Ensure Data Integrity**: Automatically detect schema changes, data type inconsistencies, and missing values to maintain the accuracy of transaction data.
- **Compliance with Standards**: Comply with regulatory requirements, including data retention, encryption, and auditability, to safeguard sensitive data.
- **Real-Time Monitoring and Alerts**: Provide real-time validation, logging, and notifications to stakeholders regarding data quality issues.
- **Scalability and Reliability**: Design the pipeline to handle large volumes of data efficiently, ensuring that the system is scalable and robust enough to meet future needs.

## Architecture Overview
The data quality pipeline consists of several AWS components working together to achieve the desired data quality goals. The architecture includes:

1. **S3 Bucket A (Raw Data Storage)**: Stores raw transaction data ingested from the KPL producer.
2. **AWS Glue Crawler**: Crawls the data in S3 Bucket A and extracts metadata to create an up-to-date schema view in the **Glue Data Catalog**.
3. **AWS Glue Data Catalog**: Stores the metadata of the datasets, including schema information that can be used for validation and reporting.
4. **AWS Glue Jobs**: Validate the ingested data to ensure compliance with data quality standards.
5. **AWS EventBridge**: Captures schema change events and routes them for further action.
6. **AWS Lambda**: Processes schema change events and triggers appropriate actions, such as alerting or logging.
7. **AWS SNS (Notification System)**: Sends notifications for critical issues detected during schema validation.
8. **S3 Bucket C (Logging Storage)**: Stores logs for data validation, alerts, and compliance.
9. **S3 Bucket B (Processed Data Storage)**: Stores processed data generated by the data analyst team using EMR (not part of this data quality pipeline).

## Workflow

### Step-by-Step Workflow
1. **Data Ingestion**: Raw transaction data is ingested and stored in **S3 Bucket A**.
2. **Schema Detection**: **AWS Glue Crawler** crawls the data in **S3 Bucket A** to extract metadata and update the **Glue Data Catalog**.
3. **Schema Validation**: **AWS Glue Jobs** validate the ingested data based on predefined quality rules.
   - **Mandatory Fields**: Checks for the presence of required fields.
   - **Data Type Validation**: Validates that each field matches the expected data type.
4. **Event Handling**: **AWS EventBridge** captures events when schema changes are detected by the Glue Crawler and routes them based on criticality.
   - **Critical Events**: Handled by **AWS Lambda** to trigger alerts through **AWS SNS**.
   - **Non-Critical Events**: Logged into **S3 Bucket C** for future reference.
5. **Alerting**: Alerts are sent via **AWS SNS** to notify data engineers of critical issues such as missing fields or data type changes.
6. **Logging**: **AWS Lambda** and **Glue Jobs** log validation results, alerts, and compliance activities in **S3 Bucket C**.

## Key Challenges
The data quality assurance pipeline is designed to address several key challenges associated with financial transaction data, such as:
- **Schema Changes**: Unexpected modifications to data structures can disrupt data processing and analytics.
- **Data Inconsistencies**: Issues such as incorrect data types, missing fields, and cross-field mismatches can lead to inaccurate insights and compliance risks.
- **Compliance Needs**: Financial data must meet stringent regulations, including data retention and encryption policies.
- **Alerting and Notification**: Quickly notifying data engineers and stakeholders when data quality issues arise is essential for timely response and resolution.

## Stakeholders
- **Data Engineering Team**: Responsible for setting up and maintaining the data quality pipeline.
- **Data Analysts**: End users who rely on accurate data for insights and decision-making.
- **Compliance Officers**: Ensure the pipeline meets regulatory and compliance requirements.
- **Business Stakeholders**: Interested in maintaining high data quality to support business processes and ensure decision-making accuracy.

## Success Criteria
The success of the data quality assurance pipeline will be measured by:
- **Reduction in Data Errors**: Fewer incidents of missing or inconsistent data in the data platform.
- **Compliance Readiness**: The system meets all regulatory requirements for data retention, security, and auditability.
- **Timely Alerts**: Real-time detection and notification of data quality issues, enabling quick remediation.
- **Scalability**: The ability of the system to scale with increasing data volumes and adapt to new data sources without significant reconfiguration.

