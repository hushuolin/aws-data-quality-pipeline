# Design and Configuration of Architecture for Fintech Data Quality Assurance Pipeline

## Overview
This document provides a detailed design and configuration of the architecture for the fintech data quality assurance pipeline. The architecture is implemented using AWS services to address the data quality requirements outlined in the Project Overview. The design ensures scalability, reliability, and compliance of the data platform.

## System Architecture
The architecture of the data quality assurance pipeline is comprised of several AWS components, each playing a specific role in maintaining data quality, ensuring compliance, and providing scalability.

### AWS Components Involved:
1. **S3 Buckets**
   - **S3 Bucket A (Raw Data Storage)**: Stores raw transaction data from the KPL producer, serving as the entry point for data processing.
   - **S3 Bucket C (Logging Storage)**: Stores logs for data validation, schema changes, and alerts, retained for auditing purposes.
   - **S3 Bucket B (Processed Data Storage)**: Stores processed data generated by data analysts using EMR (not directly involved in the data quality pipeline).

2. **AWS Glue Crawler and Glue Data Catalog**
   - **AWS Glue Crawler**: Crawls the data in S3 Bucket A to automatically extract metadata and update the **Glue Data Catalog**.
   - **Glue Data Catalog**: Stores metadata of ingested datasets, including schema information used for validation and reporting.

3. **AWS Glue Jobs**
   - **Schema Validation**: Glue Jobs validate ingested data according to predefined data quality rules:
     - **Mandatory Fields**: Ensures all required fields are present.
     - **Data Type Validation**: Ensures fields match expected data types.
     - **Cross-Field Consistency**: Validates relationships between related fields, such as `Merchant-ID` and `Merchant-Name`.

4. **AWS EventBridge**
   - **Schema Change Events**: Captures events when schema changes are detected and routes them for further action.
   - **Event Classification**: Events are classified as critical or non-critical based on severity.

5. **AWS Lambda**
   - **Event Processing**: Lambda functions handle critical events, such as missing fields or data type mismatches.
   - **Alert Triggering**: Triggers alerts via AWS SNS for critical data quality issues.
   - **Logging**: Logs validation outcomes to S3 Bucket C for compliance and audit purposes.

6. **AWS SNS (Simple Notification Service)**
   - **Critical Alert Notifications**: Notifies stakeholders, such as data engineers, whenever critical data quality issues arise.
   - **Notification Channels**: Alerts are sent via email or SMS.

## Workflow
The workflow ensures data quality issues are detected and addressed in real-time:

1. **Data Ingestion**: Raw transaction data is ingested into **S3 Bucket A**.
2. **Schema Detection**: **AWS Glue Crawler** extracts metadata and updates the **Glue Data Catalog**.
3. **Data Validation**: **AWS Glue Jobs** validate data, checking for mandatory fields, data types, and cross-field consistency.
4. **Event Handling**: **AWS EventBridge** routes events:
   - **Critical Events**: Trigger **AWS Lambda** to initiate alerts via **AWS SNS**.
   - **Non-Critical Events**: Logged in **S3 Bucket C** for audits.
5. **Alerting**: **AWS SNS** sends critical alerts for immediate action.
6. **Logging**: **AWS Lambda** and **Glue Jobs** log all activities to **S3 Bucket C** for compliance.

## Configuration Details

### 1. AWS Glue Crawler
- **Frequency**: Runs every hour to ensure up-to-date schema information.
- **Configuration**:
  - **Crawler Targets**: Configured to crawl **S3 Bucket A** for raw transaction data.
  - **Data Classifiers**: Uses default classifiers for common file types, with custom classifiers for specific data formats.
- **Output**: Metadata updates are captured in the **Glue Data Catalog**.

### 2. AWS Glue Jobs
- **Validation Logic**:
  - **Mandatory Fields**: Configured to verify the presence of required fields (`Profile-ID`, `Consumer-ID`, etc.) against the schema defined in the **Glue Data Catalog**.
  - **Data Type Checks**: Uses custom scripts to ensure data types are correct, e.g., `Transaction-Amount` must be of type float.
  - **Cross-Field Relationships**: Validates relationships between fields such as `Merchant-ID` and `Merchant-Name` using business rules.
- **Job Scheduling**: Triggered upon data ingestion completion or on a scheduled basis to maintain data quality.
- **Error Handling**: Validation errors are logged, and failed records are written to **S3 Bucket C** for analysis.

### 3. AWS Lambda Functions
- **Processing Logic**:
  - **Critical Issue Handling**: Configured to handle critical schema changes and data quality issues by triggering an SNS alert.
  - **Event Sources**: Subscribed to **AWS EventBridge** to respond to schema change events and validation issues.
  - **Logging**: Validation outcomes, including successful and failed checks, are written to **S3 Bucket C**.
- **Retries and Error Handling**: Lambda is configured with retry logic for transient errors and writes error details to CloudWatch.

### 4. AWS EventBridge
- **Event Rules**:
  - **Schema Change Detection**: Captures changes detected by the Glue Crawler and routes them based on their severity.
  - **Event Routing**: Routes critical schema change events to **AWS Lambda** and logs non-critical events to **S3 Bucket C**.
- **Frequency**: Monitors schema changes in real-time to ensure immediate action.

### 5. AWS SNS
- **Alert Configuration**:
  - **Notification Channels**: Alerts are configured to be sent via email and SMS to data engineers and compliance officers.
  - **Escalation Policy**: If issues are not addressed within a specified timeframe, SNS escalates by notifying higher-level stakeholders.
  - **Message Content**: Alerts include detailed information on the data quality issue, such as affected fields and recommended actions.

## Scalability and Performance Considerations
- **Serverless Architecture**: AWS Glue, Lambda, and SNS ensure the pipeline can scale based on data volume.
- **Monitoring**: AWS CloudWatch monitors performance metrics, including execution times and resource utilization, to detect and mitigate bottlenecks.
- **High-Volume Testing**: The pipeline is tested using high-volume datasets (e.g., 100,000+ records) to validate performance under peak loads and ensure efficient resource allocation.

## Security and Compliance
- **Data Encryption**: Data in S3 is encrypted at rest (using AWS KMS) and in transit (using SSL/TLS).
- **Access Control**: IAM roles and policies enforce role-based access control (RBAC) to ensure that only authorized personnel have access to sensitive data.
- **Audit Logs**: All validation activities are logged to **S3 Bucket C** for compliance purposes, with logs retained for 7 years as per regulatory requirements.
- **Data Masking**: Personally identifiable information (PII) is masked during validation to protect customer privacy and meet compliance standards.

## Summary
The architecture for the data quality assurance pipeline meets requirements for data integrity, compliance, and scalability. The integration of AWS Glue, Lambda, EventBridge, SNS, and S3 ensures data quality is managed effectively. By automating schema detection, validation, alerting, and logging, the architecture provides a comprehensive solution for maintaining high-quality data in the fintech data platform.

